{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["This notebook contains functionality to perform the following:\n","\n","Implementing and testing XGBoost functionality trained on the 1/16th arabidopsis (i)starr dataset."],"metadata":{"id":"Rk_EdAsQQ6xq"}},{"cell_type":"code","metadata":{"id":"6edXaVh6UQrB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1642190669237,"user_tz":420,"elapsed":1190,"user":{"displayName":"Sacha Davis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGR8B7037KwyLJZN5q6c5LXQBl6nrPG12AQfIZvQ=s64","userId":"18177606934486709137"}},"outputId":"e9a0c75b-f3a7-47c2-80c0-f3efa24ba7a6"},"source":["# mount google drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd '/content/drive/Shareddrives/NRC_Amii_Agronomics_Project/nrc-ml-plant-genomics/'"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/Shareddrives/NRC_Amii_Agronomics_Project/nrc-ml-plant-genomics\n"]}]},{"cell_type":"code","metadata":{"id":"ydWkitG9U4Wh"},"source":["import keras\n","import warnings, logging\n","import json\n","import numpy as np\n","import pandas as pd\n","import datetime, os\n","\n","from keras.models import Sequential, load_model, model_from_json\n","# from keras.layers import Input, Dense, Conv1D, MaxPooling2D, Dropout, Flatten, BatchNormalization\n","from keras.callbacks import ModelCheckpoint, EarlyStopping  # https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n","import tensorflow as tf\n","\n","from keras import layers\n","from keras import activations\n","\n","import xgboost\n","\n","from sklearn.metrics import r2_score\n","from scipy.stats import spearmanr # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html\n","\n","warnings.filterwarnings('ignore')\n","logging.disable(1000)\n","\n","%load_ext tensorboard"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZzryDgljU8cr"},"source":["args = {\"data_path\":\"data/processed/arabidopsis_every_16.csv\",\n","        \"include_mononuc_freq\":1,\n","        \"include_dinuc_freq\":0,\n","        \"include_trinuc_freq\":0,\n","        \"include_starr\":1,\n","        \"include_istarr\":0,\n","        \"shuffle\":0}\n","\n","nts = [\"A\", \"T\", \"C\", \"G\"]  # list of single nucleotides\n","\n","mapping = {\"A\": [1.0, 0.0, 0.0, 0.0], \"T\": [0.0, 0.0, 0.0, 1.0], \"C\": [0.0, 1.0, 0.0, 0.0], \"G\": [0.0, 0.0, 1.0, 0.0]}  # cross referenced with kipoi data loader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ubr_IqRVE8t"},"source":["def get_model(args):\n","  return xgboost.XGBRegressor()\n","\n","\n","def get_ohe(sequence):  # gets sequence in format model can use (145, 4)\n","    return np.array([mapping[nt] for nt in sequence])\n","\n","\n","def train_test_val(args):\n","    include = []  # captures all sequences we are including as input features\n","    if args[\"include_mononuc_freq\"] == 1:  include += nts\n","    if args[\"include_dinuc_freq\"] == 1:    include += [nt1+nt2 for nt1 in nts for nt2 in nts]\n","    if args[\"include_trinuc_freq\"] == 1:   include += [nt1+nt2+nt3 for nt1 in nts for nt2 in nts for nt3 in nts]\n","\n","    df = pd.read_csv(args[\"data_path\"])\n","    print(\"read csv\")\n","\n","    # freqs = []  # to hold the frequencies of each sequence  [df.shape[0], len(include)]\n","    # for i in range(df.shape[0]):\n","    #     sqnc_freq = []\n","    #     for item in include:  # for each sequence we care about, count how often it appears\n","    #         sqnc_freq.append(df.loc[i, \"sequence\"].count(item))\n","    #     freqs.append(sqnc_freq)\n","    # df[include] = freqs  # creates new columns in dataframe\n","\n","    for item in include:  # create new columns with the counts of sequences in \"include\"\n","      print(\"including\", item)\n","      df[item] = df.sequence.str.count(item)\n","\n","    target = \"target_istarr\" if \"include_starr\" == 1 else \"target_starr\"\n","\n","    train_df = df[df.set == \"train\"]\n","    X_train = np.array(train_df[include])\n","    y_train = np.array(train_df[target].tolist())\n","\n","    val_df = df[df.set == \"val\"]\n","    X_val = np.array(val_df[include])\n","    y_val = np.array(val_df[target].tolist())\n","\n","    test_df = df[df.set == \"test\"]\n","    X_test = np.array(test_df[include])\n","    y_test = np.array(test_df[target].tolist())\n","\n","    return X_train, y_train, X_val, y_val, X_test, y_test\n","\n","\n","def return_y(args, df):  # based on what to include, returns y array\n","    if args[\"include_starr\"]:\n","      if args[\"include_istarr\"]:\n","        y = np.array(pd.concat([df[\"target_starr\"], df[\"target_istarr\"]], axis=1))\n","      else:\n","        y = np.array(df[\"target_starr\"].tolist())\n","    else:\n","      y = np.array(df[\"target_istarr\"].tolist())\n","\n","    return y\n","\n","\n","def save_results(args, dir_path, X_train, X_test, X_val, y_train, y_test, y_val, saved_model):\n","    if args[\"include_starr\"] + args[\"include_istarr\"] == 2:\n","\n","      train_predictions = saved_model.predict(X_train)\n","      val_predictions = saved_model.predict(X_val)\n","      test_predictions = saved_model.predict(X_test)\n","\n","      starr_scores = [[str(r2_score(y_train[:,0], train_predictions[:,0])),\n","                       str(r2_score(y_val[:,0], val_predictions[:,0])),\n","                       str(r2_score(y_test[:,0], test_predictions[:,0]))],\n","                      [str(spearmanr(y_train[:,0], train_predictions[:,0])[0]),\n","                       str(spearmanr(y_val[:,0], val_predictions[:,0])[0]),\n","                       str(spearmanr(y_test[:,0], test_predictions[:,0])[0])]]\n","\n","      istarr_scores = [[str(r2_score(y_train[:,1], train_predictions[:,1])),\n","                        str(r2_score(y_val[:,1], val_predictions[:,1])),\n","                        str(r2_score(y_test[:,1], test_predictions[:,1]))],\n","                       [str(spearmanr(y_train[:,1], train_predictions[:,1])[0]),\n","                        str(spearmanr(y_val[:,1], val_predictions[:,1])[0]),\n","                        str(spearmanr(y_test[:,1], test_predictions[:,1])[0])]]\n","\n","      # write r2 and spearman scores for all of train, test, and val sets and starr & istarr\n","      write_results_to_file(dir_path+\"/results_starr.csv\", starr_scores)\n","      write_results_to_file(dir_path+\"/results_istarr.csv\", istarr_scores)\n","\n","    else:\n","      # calculate all scores from\n","      scores = [[str(r2_score(y_train, saved_model.predict(X_train).reshape(1, -1)[0])),\n","                 str(r2_score(y_val, saved_model.predict(X_val).reshape(1, -1)[0])),\n","                 str(r2_score(y_test, saved_model.predict(X_test).reshape(1, -1)[0]))\n","                 ],\n","                [str(spearmanr(y_train, saved_model.predict(X_train).reshape(1, -1)[0])[0]),\n","                 str(spearmanr(y_val, saved_model.predict(X_val).reshape(1, -1)[0])[0]),\n","                 str(spearmanr(y_test, saved_model.predict(X_test).reshape(1, -1)[0])[0])\n","                ]]\n","\n","      # write r2 and spearman scores for all of train, test, and val sets\n","      write_results_to_file(dir_path+\"/results.csv\", scores)\n","\n","\n","def write_results_to_file(path, scores):\n","    # creates a file at path address, writes scores from scores nested list to output\n","    with open(path, \"w\") as f:\n","      f.write(\",train,val,test\\n\")\n","      f.write(\"r2,\"+scores[0][0]+\",\"+scores[0][1]+\",\"+scores[0][2]+\"\\n\")\n","      f.write(\"spearman,\"+scores[1][0]+\",\"+scores[1][1]+\",\"+scores[1][2]+\"\\n\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WRkw45o2dXez","executionInfo":{"status":"ok","timestamp":1642191182589,"user_tz":420,"elapsed":39401,"user":{"displayName":"Sacha Davis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGR8B7037KwyLJZN5q6c5LXQBl6nrPG12AQfIZvQ=s64","userId":"18177606934486709137"}},"outputId":"aec61590-0fb4-46b1-93ac-ca3860c7e89a"},"source":["X_train, y_train, X_val, y_val, X_test, y_test = train_test_val(args)  # get dataset\n","\n","# # models\n","model = get_model(args)  # instantiate and init model\n","\n","# # create path to folder with results \n","date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","# arch_settings = str(args.conv_one_set)+str(args.conv_two_set)+str(args.conv_three_set)+str(args.linear_mapping)\n","# dir_path = \"experiments/exp_\"+date+\"_\"+arch_settings+\"_lr\"+str(args.learning_rate)+\"_bs\"+str(args.batch_size)+\"_ep\"+str(args.num_epochs)\n","dir_path = \"experiments/xgboost\"+date\n","!mkdir {dir_path}\n","# # compile model\n","# # model.compile(optimizer=Adam(lr=args.learning_rate),  # CHANGE IF WE WANT TO CHANGE OPTIM\n","# #               loss='mean_squared_error')\n","# model.compile(loss='mean_squared_error', optimizer='adam')\n","\n","\n","# # init callbacks\n","# logdir = os.path.join(dir_path, \"logs\")\n","# tensorboard_callback = keras.callbacks.TensorBoard(logdir, histogram_freq=1)  # https://stackoverflow.com/questions/59894720/keras-and-tensorboard-attributeerror-sequential-object-has-no-attribute-g\n","# es_callback = EarlyStopping(monitor='val_loss', verbose=1, patience=20, min_delta=0.001)\n","# mc_callback = ModelCheckpoint(dir_path+'/best_model.h5', monitor='val_loss', save_best_only=True)\n","\n","# train model\n","model.fit(X_train, y_train)\n","\n","# save training history\n","# hist_df = pd.DataFrame(history.history) \n","# hist_df.to_csv(dir_path+'/training_history.csv')\n","\n","# load best model according to val_loss\n","# saved_model = load_model(dir_path+'/best_model.h5')\n","\n","# write r2 and spearman scores for all of train, test, and val sets\n","save_results(args, dir_path, X_train, X_test, X_val, y_train, y_test, y_val, model)\n","\n","# write all args to text file for reproducibility \n","# json.dump(vars(args), open(dir_path+\"/settings.txt\", \"w\"))  # https://www.kite.com/python/answers/how-to-save-a-dictionary-to-a-file-in-python\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["read csv\n","including A\n","including T\n","including C\n","including G\n","[20:12:35] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"]}]},{"cell_type":"code","metadata":{"id":"g1NeUNw_wuiz"},"source":["args = {\"data_path\":\"data/processed/arabidopsis_every_16.csv\",\n","        \"include_starr\":1,\n","        \"include_istarr\":0,\n","        \"shuffle\":0}\n","\n","mapping = {\"A\": [1.0, 0.0, 0.0, 0.0], \"T\": [0.0, 0.0, 0.0, 1.0], \"C\": [0.0, 1.0, 0.0, 0.0], \"G\": [0.0, 0.0, 1.0, 0.0]}  # cross referenced with kipoi data loader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rwmy0K2UwyRd","executionInfo":{"status":"ok","timestamp":1638764517365,"user_tz":420,"elapsed":1711714,"user":{"displayName":"Sarah Davis","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhGR8B7037KwyLJZN5q6c5LXQBl6nrPG12AQfIZvQ=s64","userId":"18177606934486709137"}},"outputId":"bbfff197-e20a-42ab-bb45-c811c2979158"},"source":["X_train, y_train, X_val, y_val, X_test, y_test = train_test_val(args)  # get dataset\n","\n","print(X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape)\n","# # models\n","model = get_model(args)  # instantiate and init model\n","\n","# # create path to folder with results \n","date = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n","# arch_settings = str(args.conv_one_set)+str(args.conv_two_set)+str(args.conv_three_set)+str(args.linear_mapping)\n","# dir_path = \"experiments/exp_\"+date+\"_\"+arch_settings+\"_lr\"+str(args.learning_rate)+\"_bs\"+str(args.batch_size)+\"_ep\"+str(args.num_epochs)\n","dir_path = \"experiments/test_starr\"\n","\n","# # compile model\n","# # model.compile(optimizer=Adam(lr=args.learning_rate),  # CHANGE IF WE WANT TO CHANGE OPTIM\n","# #               loss='mean_squared_error')\n","model.compile(loss='mean_squared_error', optimizer='adam')\n","\n","\n","# # init callbacks\n","logdir = os.path.join(dir_path, \"logs\")\n","tensorboard_callback = keras.callbacks.TensorBoard(logdir, histogram_freq=1)  # https://stackoverflow.com/questions/59894720/keras-and-tensorboard-attributeerror-sequential-object-has-no-attribute-g\n","es_callback = EarlyStopping(monitor='val_loss', verbose=1, patience=20, min_delta=0.001)\n","mc_callback = ModelCheckpoint(dir_path+'/best_model.h5', monitor='val_loss', save_best_only=True)\n","\n","# train model\n","history = model.fit(X_train, y_train,\n","                    epochs=100,\n","                    batch_size=512,\n","                    validation_data=(X_val, y_val),\n","                    callbacks=[tensorboard_callback, es_callback, mc_callback])\n","\n","# save training history\n","hist_df = pd.DataFrame(history.history) \n","hist_df.to_csv(dir_path+'/training_history.csv')\n","\n","# load best model according to val_loss\n","saved_model = load_model(dir_path+'/best_model.h5')\n","\n","# write r2 and spearman scores for all of train, test, and val sets\n","save_results(args, dir_path, X_train, X_test, X_val, y_train, y_test, y_val, saved_model)\n","\n","# write all args to text file for reproducibility \n","# json.dump(vars(args), open(dir_path+\"/settings.txt\", \"w\"))  # https://www.kite.com/python/answers/how-to-save-a-dictionary-to-a-file-in-python\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(994521, 7)\n","(677510, 145, 4) (677510,) (161051, 145, 4) (161051,) (155960, 145, 4) (155960,)\n","Epoch 1/100\n","1324/1324 [==============================] - 30s 22ms/step - loss: 0.2672 - val_loss: 0.2398\n","Epoch 2/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2529 - val_loss: 0.2373\n","Epoch 3/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2515 - val_loss: 0.2359\n","Epoch 4/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2926 - val_loss: 0.2968\n","Epoch 5/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2753 - val_loss: 0.2473\n","Epoch 6/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2603 - val_loss: 0.2380\n","Epoch 7/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2542 - val_loss: 0.2358\n","Epoch 8/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2501 - val_loss: 0.2429\n","Epoch 9/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2524 - val_loss: 0.2893\n","Epoch 10/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2528 - val_loss: 0.2355\n","Epoch 11/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2465 - val_loss: 0.2372\n","Epoch 12/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2445 - val_loss: 0.2355\n","Epoch 13/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2441 - val_loss: 0.2319\n","Epoch 14/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2426 - val_loss: 0.2336\n","Epoch 15/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2427 - val_loss: 0.2341\n","Epoch 16/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2406 - val_loss: 0.2310\n","Epoch 17/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2395 - val_loss: 0.2302\n","Epoch 18/100\n","1324/1324 [==============================] - 28s 22ms/step - loss: 0.2404 - val_loss: 0.2295\n","Epoch 19/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2386 - val_loss: 0.2307\n","Epoch 20/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2389 - val_loss: 0.2281\n","Epoch 21/100\n","1324/1324 [==============================] - 28s 22ms/step - loss: 0.2332 - val_loss: 0.2269\n","Epoch 22/100\n","1324/1324 [==============================] - 29s 22ms/step - loss: 0.2310 - val_loss: 0.2264\n","Epoch 23/100\n","1324/1324 [==============================] - 29s 22ms/step - loss: 0.2305 - val_loss: 0.2261\n","Epoch 24/100\n","1324/1324 [==============================] - 28s 22ms/step - loss: 0.2286 - val_loss: 0.2261\n","Epoch 25/100\n","1324/1324 [==============================] - 29s 22ms/step - loss: 0.2278 - val_loss: 0.2255\n","Epoch 26/100\n","1324/1324 [==============================] - 29s 22ms/step - loss: 0.2270 - val_loss: 0.2249\n","Epoch 27/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2260 - val_loss: 0.2260\n","Epoch 28/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2251 - val_loss: 0.2266\n","Epoch 29/100\n","1324/1324 [==============================] - 29s 22ms/step - loss: 0.2240 - val_loss: 0.2240\n","Epoch 30/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2231 - val_loss: 0.2246\n","Epoch 31/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2223 - val_loss: 0.2243\n","Epoch 32/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2212 - val_loss: 0.2243\n","Epoch 33/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2206 - val_loss: 0.2246\n","Epoch 34/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2195 - val_loss: 0.2261\n","Epoch 35/100\n","1324/1324 [==============================] - 28s 22ms/step - loss: 0.2187 - val_loss: 0.2232\n","Epoch 36/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2176 - val_loss: 0.2259\n","Epoch 37/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2170 - val_loss: 0.2239\n","Epoch 38/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2160 - val_loss: 0.2257\n","Epoch 39/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2152 - val_loss: 0.2278\n","Epoch 40/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2142 - val_loss: 0.2281\n","Epoch 41/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2143 - val_loss: 0.2260\n","Epoch 42/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2126 - val_loss: 0.2261\n","Epoch 43/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2120 - val_loss: 0.2259\n","Epoch 44/100\n","1324/1324 [==============================] - 28s 22ms/step - loss: 0.2110 - val_loss: 0.2267\n","Epoch 45/100\n","1324/1324 [==============================] - 28s 22ms/step - loss: 0.2103 - val_loss: 0.2261\n","Epoch 46/100\n","1324/1324 [==============================] - 29s 22ms/step - loss: 0.2096 - val_loss: 0.2282\n","Epoch 47/100\n","1324/1324 [==============================] - 29s 22ms/step - loss: 0.2088 - val_loss: 0.2262\n","Epoch 48/100\n","1324/1324 [==============================] - 28s 21ms/step - loss: 0.2080 - val_loss: 0.2293\n","Epoch 49/100\n","1324/1324 [==============================] - 28s 22ms/step - loss: 0.2075 - val_loss: 0.2268\n","Epoch 00049: early stopping\n"]}]},{"cell_type":"code","metadata":{"id":"Glp0RjOZyBon"},"source":[],"execution_count":null,"outputs":[]}]}