{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["This notebook contains functionality to perform the following:\n","\n","Load in models trained using various street-lit (or not) datasets, and test on various test sets (including both accessible and inaccessible). Output .csv with summary of performance."],"metadata":{"id":"LXO50KXwqSg8"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ekhhnVOU6_XW","executionInfo":{"status":"ok","timestamp":1671570040453,"user_tz":300,"elapsed":28968,"user":{"displayName":"Sacha Davis","userId":"18177606934486709137"}},"outputId":"dc47a9f8-ef53-4fd3-e6bd-4012648456fb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/Shareddrives/NRC_Amii_Agronomics_Project/nrc-ml-plant-genomics\n"]}],"source":["# mount google drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","%cd '/content/drive/Shareddrives/NRC_Amii_Agronomics_Project/nrc-ml-plant-genomics/'"]},{"cell_type":"code","source":["# imports\n","import argparse\n","import keras\n","import warnings, logging\n","import numpy as np\n","import pandas as pd\n","import datetime, time, os\n","import json\n","import random\n","import tensorflow as tf\n","import math\n","import glob\n","import ast\n","\n","from keras.models import Sequential, load_model, model_from_json\n","from keras.layers import Input, Dense, Conv1D, MaxPooling2D, Dropout, Flatten, BatchNormalization\n","from tensorflow.keras.optimizers import Adam  # https://stackoverflow.com/questions/62707558/importerror-cannot-import-name-adam-from-keras-optimizers\n","from keras.callbacks import ModelCheckpoint, EarlyStopping  # https://machinelearningmastery.com/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping/\n","from collections import Counter\n","\n","from sklearn.metrics import r2_score, accuracy_score\n","from scipy.stats import spearmanr  # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html\n","\n","warnings.filterwarnings('ignore')\n","logging.disable(1000)\n","\n","# tf.random.set_seed(1202)  # https://www.tensorflow.org/api_docs/python/tf/random/set_seed\n","# from numpy.random import seed\n","# seed(1202)\n","\n","random.seed(1234)\n","\n","nts = [\"A\", \"T\", \"C\", \"G\"]  # list of single nucleotides\n","mapping = {\"A\": [1, 0, 0, 0], \"T\": [0, 0, 0, 1], \"C\": [0, 1, 0, 0], \"G\": [0, 0, 1, 0], \"X\":[0, 0, 0, 0]}  # cross referenced with kipoi data loader\n","\n","def Spearman(y_true, y_pred):\n","     return (tf.py_function(spearmanr, [tf.cast(y_pred, tf.float32), \n","                       tf.cast(y_true, tf.float32)], Tout = tf.float32) )"],"metadata":{"id":"dN_BIrXx_GWG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Old 35s"],"metadata":{"id":"Eo2CF5HcroBZ"}},{"cell_type":"code","source":["# define models\n","\n","# simple models and complex models\n","def freq_model_architecture(args, in_dim):  # initializes model architecture\n","    mdl = Sequential()\n","\n","    # this is the only layer that is enforced. to test linear regression only, set layer_1_size to 1 and layer_1_activation to \"linear\"\n","    mdl.add(Dense(args[\"layer_1_size\"], input_dim=in_dim, activation=args[\"layer_1_activation\"]))\n","\n","    if args[\"layer_2_size\"] > 0:       mdl.add(Dense(args[\"layer_2_size\"], activation=args[\"layer_2_activation\"]))\n","    if args[\"layer_3_size\"] > 0:       mdl.add(Dense(args[\"layer_3_size\"], activation=args[\"layer_3_activation\"]))\n","    if args[\"output_layer_size\"] > 0:  mdl.add(Dense(args[\"output_layer_size\"], activation=args[\"output_layer_activation\"]))\n","\n","    return mdl\n","\n","# models without linear mapping & models with linear mapping\n","def conv_model_architecture(args):  # initializes model architecture\n","    mdl = Sequential()\n","\n","    conv1_train = args[\"conv_one_set\"] != 2  # True if conv layer should be trained\n","    mdl.add(Conv1D(120, 5, activation='relu', input_shape=(args[\"input_sequence_length\"], 4), name=\"1DConv_1\", trainable=conv1_train))\n","    mdl.add(BatchNormalization(name=\"batchNorm1\", trainable=conv1_train))\n","    mdl.add(Dropout(0.1, name=\"drop1\"))\n","\n","    conv2_train = args[\"conv_two_set\"] != 2  # True if conv layer should be trained\n","    mdl.add(Conv1D(120, 5, activation='relu', name=\"1DConv_2\", trainable=conv2_train))\n","    mdl.add(BatchNormalization(name=\"batchNorm2\", trainable=conv2_train))\n","    mdl.add(Dropout(0.1, name=\"drop2\"))\n","\n","    if args[\"last_conv_layer\"] == 1:  # if we are not removing last conv layer for simplicity\n","      conv3_train = args[\"conv_three_set\"] != 2  # True if conv layer should be trained\n","      mdl.add(Conv1D(120, 5, activation='relu', name=\"1DConv_3\", trainable=conv3_train))\n","      mdl.add(BatchNormalization(name=\"batchNorm3\", trainable=conv3_train))\n","      mdl.add(Dropout(0.1, name=\"drop3\"))\n","\n","    mdl.add(Flatten(name=\"flat\"))\n","\n","    if args[\"linear_mapping\"] == 1: \n","        mdl.add(Dense(12, activation='linear', name=\"dense1\", trainable=False))\n","\n","    # output layer\n","    mdl.add(Dense(1, activation=\"linear\", name=\"dense2\"))\n","\n","    return mdl"],"metadata":{"id":"t2hO9WSo_ckr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# functions for getting data into proper format\n","\n","def prepare_validation_freq(args, df):\n","    include = []  # captures all sequences we are including as input features\n","\n","    if args[\"include_mononuc_freq\"] == 1:  include += nts\n","    if args[\"include_dinuc_freq\"] == 1:    include += [nt1+nt2 for nt1 in nts for nt2 in nts]\n","    if args[\"include_trinuc_freq\"] == 1:   include += [nt1+nt2+nt3 for nt1 in nts for nt2 in nts for nt3 in nts]\n","\n","    X_test = np.array(df[include])\n","    y_test = np.array(df[\"target\"].tolist())\n","\n","    return X_test, y_test\n","\n","def prepare_validation_conv(df):\n","    X_test = np.array([get_ohe(sqnc) for sqnc in df[\"sequence\"]])\n","    y_test = np.array(df[\"target\"].tolist())\n","\n","    return X_test, y_test\n","\n","def get_ohe(sequence):  # gets sequence in format model can use (x, 4)\n","    return np.array([mapping[nt] for nt in sequence])"],"metadata":{"id":"0MByRwGeMYjU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load in 35S data\n","thirty_five_s = pd.read_csv(\"data/processed/validation.csv\")\n","\n","nts = [\"A\", \"T\", \"C\", \"G\"]  # list of single nucleotides\n","include = [] + nts\n","include += [nt1+nt2 for nt1 in nts for nt2 in nts]\n","include += [nt1+nt2+nt3 for nt1 in nts for nt2 in nts for nt3 in nts]\n","\n","for item in include:  # create new columns with the counts of sequences in \"include\"\n","  # print(\"including\", item)\n","  thirty_five_s[item] = thirty_five_s.sequence.str.count(item)"],"metadata":{"id":"puHAwk19_aIe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["experiment_paths = []\n","results_on_35s_r2 = []\n","results_on_35s_spear = []\n","\n","tester = []\n","\n","for path in glob.glob(\"experiments/streetlight/old/*/*\"):\n","  if \"napus_models\" in path:\n","    pass  # we don't want to do any evaluation here\n","\n","  else:\n","    with open(path+'/settings.txt') as f:  # load in settings\n","      args = ast.literal_eval(f.read())\n","\n","    # print(path)\n","\n","    if \"nucfreq\" in path:  # frequency based model\n","      X_35s, y_35s = prepare_validation_freq(args, thirty_five_s)\n","      mdl = freq_model_architecture(args, X_35s.shape[1])\n","    else:  # conv based model\n","      X_35s, y_35s = prepare_validation_conv(thirty_five_s)\n","      mdl = conv_model_architecture(args)\n","\n","    mdl.load_weights(path+'/best_model.h5')\n","\n","    experiment_paths.append(path)\n","\n","    y_35s_pred = mdl.predict(X_35s)\n","    results_on_35s_r2.append(r2_score(y_35s, y_35s_pred.reshape(1, -1)[0]))\n","    results_on_35s_spear.append(spearmanr(y_35s, y_35s_pred.reshape(1, -1)[0])[0])"],"metadata":{"id":"GKtpeNdHBZTe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# create results dataframe\n","results_df = pd.DataFrame()\n","results_df[\"filepath\"] = experiment_paths\n","results_df[\"r2\"] = results_on_35s_r2\n","results_df[\"spearman\"] = results_on_35s_spear"],"metadata":{"id":"VwK7wZoVDnGM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_df"],"metadata":{"id":"o96RygdXD8ps"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results_df.spearman.max()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J3iJ6sZaTqcY","executionInfo":{"status":"ok","timestamp":1666654599689,"user_tz":240,"elapsed":130,"user":{"displayName":"Sacha Davis","userId":"10551420809672051014"}},"outputId":"9ac1d151-9c46-49aa-80a3-0296ac45448a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.11736842623004673"]},"metadata":{},"execution_count":71}]},{"cell_type":"code","source":["# save dataframe\n","results_df.to_csv(\"experiments/streetlight/results_35s.csv\", index=False)"],"metadata":{"id":"7qCbZKqwD03a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Accessible, Inaccessible, and 35S"],"metadata":{"id":"MPaQpyuRrqaL"}},{"cell_type":"code","source":["# define models\n","\n","# simple models and complex models\n","def freq_model_architecture(args, in_dim):  # initializes model architecture\n","    mdl = Sequential()\n","\n","    # this is the only layer that is enforced. to test linear regression only, set layer_1_size to 1 and layer_1_activation to \"linear\"\n","    mdl.add(Dense(args[\"layer_1_size\"], input_dim=in_dim, activation=args[\"layer_1_activation\"]))\n","\n","    if args[\"layer_2_size\"] > 0:       mdl.add(Dense(args[\"layer_2_size\"], activation=args[\"layer_2_activation\"]))\n","    if args[\"layer_3_size\"] > 0:       mdl.add(Dense(args[\"layer_3_size\"], activation=args[\"layer_3_activation\"]))\n","    if args[\"output_layer_size\"] > 0:  mdl.add(Dense(args[\"output_layer_size\"], activation=args[\"output_layer_activation\"]))\n","\n","    return mdl\n","\n","# models without linear mapping & models with linear mapping\n","def conv_model_architecture(args):  # initializes model architecture\n","    mdl = Sequential()\n","\n","    conv1_train = args[\"conv_one_set\"] != 2  # True if conv layer should be trained\n","    mdl.add(Conv1D(120, 5, activation='relu', input_shape=(args[\"input_sequence_length\"], 4), name=\"1DConv_1\", trainable=conv1_train))\n","    mdl.add(BatchNormalization(name=\"batchNorm1\", trainable=conv1_train))\n","    mdl.add(Dropout(0.1, name=\"drop1\"))\n","\n","    conv2_train = args[\"conv_two_set\"] != 2  # True if conv layer should be trained\n","    mdl.add(Conv1D(120, 5, activation='relu', name=\"1DConv_2\", trainable=conv2_train))\n","    mdl.add(BatchNormalization(name=\"batchNorm2\", trainable=conv2_train))\n","    mdl.add(Dropout(0.1, name=\"drop2\"))\n","\n","    if args[\"last_conv_layer\"] == 1:  # if we are not removing last conv layer for simplicity\n","      conv3_train = args[\"conv_three_set\"] != 2  # True if conv layer should be trained\n","      mdl.add(Conv1D(120, 5, activation='relu', name=\"1DConv_3\", trainable=conv3_train))\n","      mdl.add(BatchNormalization(name=\"batchNorm3\", trainable=conv3_train))\n","      mdl.add(Dropout(0.1, name=\"drop3\"))\n","\n","    mdl.add(Flatten(name=\"flat\"))\n","\n","    if args[\"linear_mapping\"] == 1: \n","        mdl.add(Dense(12, activation='linear', name=\"dense1\", trainable=False))\n","\n","    # output layer\n","    mdl.add(Dense(1, activation=\"linear\", name=\"dense2\"))\n","\n","    return mdl"],"metadata":{"id":"NlsH_4twbaAE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# functions for getting data into proper format\n","\n","def prepare_validation_freq(args, df):\n","    include = []  # captures all sequences we are including as input features\n","\n","    if args[\"include_mononuc_freq\"] == 1:  include += nts\n","    if args[\"include_dinuc_freq\"] == 1:    include += [nt1+nt2 for nt1 in nts for nt2 in nts]\n","    if args[\"include_trinuc_freq\"] == 1:   include += [nt1+nt2+nt3 for nt1 in nts for nt2 in nts for nt3 in nts]\n","\n","    X_test = np.array(df[include])\n","    y_test = np.array(df[\"target\"].tolist())\n","\n","    return X_test, y_test\n","\n","def prepare_validation_conv(df):\n","    X_test = np.array([get_ohe(sqnc) for sqnc in df[\"sequence\"]])\n","    y_test = np.array(df[\"target\"].tolist())\n","\n","    return X_test, y_test\n","\n","def get_ohe(sequence):  # gets sequence in format model can use (x, 4)\n","    return np.array([mapping[nt] for nt in sequence])\n","\n","\n","def create_freq_features(df):\n","  nts = [\"A\", \"T\", \"C\", \"G\"]  # list of single nucleotides\n","  include = [] + nts\n","  include += [nt1+nt2 for nt1 in nts for nt2 in nts]\n","  include += [nt1+nt2+nt3 for nt1 in nts for nt2 in nts for nt3 in nts]\n","\n","  for item in include:  # create new columns with the counts of sequences in \"include\"\n","    # print(\"including\", item)\n","    df[item] = df.sequence.str.count(item)\n","\n","  return df"],"metadata":{"id":"CWYjyhjcrttE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load in accessible data & restrict to test set\n","\n","test_accessible = pd.read_csv(\"data/processed/athal_accessible.csv\")\n","test_accessible = create_freq_features(test_accessible[test_accessible[\"set\"] == \"test\"])"],"metadata":{"id":"EIzC80ber_3v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load in inaccessible data & restrict to test set\n","\n","test_inaccessible = pd.read_csv(\"data/processed/athal_inaccessible.csv\")\n","test_inaccessible = create_freq_features(test_inaccessible[test_inaccessible[\"set\"] == \"test\"])"],"metadata":{"id":"3ec4GKqasZCE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load in 35s\n","\n","test_35s = pd.read_csv(\"data/processed/validation.csv\")\n","test_35s = create_freq_features(test_35s)"],"metadata":{"id":"-UHAPRlRsb2a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# define results df structure - four models on three test datasets\n","\n","# columns = model_filepath, test_dataset, r2, spearman"],"metadata":{"id":"o527iPVCtnIg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_r2_results = []\n","total_spearman_results = []\n","experiment_paths = []\n","for path in glob.glob(\"experiments/streetlight/conclusive/models/*\"):\n","  # get settings\n","  with open(path+'/settings.txt') as f:  # one model (either freq or conv)\n","    args = ast.literal_eval(f.read())\n","  \n","  experiment_paths.append(path.split(\"/\")[-1])  # we should end up with four of these\n","  print(path.split(\"/\")[-1])  \n","\n","  # test model on sets\n","  model_r2_results = []\n","  model_spearman_results = []\n","  model_paths = []\n","  for test_set in [test_accessible, test_inaccessible, test_35s]:\n","\n","    if \"freq\" in path:  # frequency based model\n","      X, y = prepare_validation_freq(args, test_set)\n","      mdl = freq_model_architecture(args, X.shape[1])\n","    else:  # conv based model\n","      X, y = prepare_validation_conv(test_set)\n","      mdl = conv_model_architecture(args)\n","    mdl.load_weights(path+'/best_weights.h5')\n","\n","    y_pred = mdl.predict(X)\n","    model_r2_results.append(r2_score(y, y_pred.reshape(1, -1)[0]))\n","\n","    spearman = spearmanr(y, y_pred.reshape(1, -1)[0])[0]\n","    print(spearman)\n","    model_spearman_results.append(spearman)\n","\n","  total_r2_results.append(model_r2_results)\n","  total_spearman_results.append(model_spearman_results)"],"metadata":{"id":"SUzUXV6psxWh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669861795642,"user_tz":300,"elapsed":90781,"user":{"displayName":"Sacha Davis","userId":"18177606934486709137"}},"outputId":"fb5cda74-cbbc-4ec9-df1a-b3b0a80dfb1e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["freq_20221130-234223_nuc101_lay24-64-0-1_lr0.002_bs512_accessible\n","698/698 [==============================] - 1s 1ms/step\n","0.38650482156087496\n","576/576 [==============================] - 1s 1ms/step\n","0.4335188858859176\n","38/38 [==============================] - 0s 1ms/step\n","0.0028904513912600613\n","freq_20221130-234316_nuc101_lay24-64-0-1_lr0.002_bs512_inaccessible\n","698/698 [==============================] - 1s 1ms/step\n","0.32456155726094166\n","576/576 [==============================] - 1s 1ms/step\n","0.5346845410150658\n","38/38 [==============================] - 0s 1ms/step\n","0.006782605863301782\n","conv_20221130-234403_0000_lr0.002_bs512_ep500_accessible\n","698/698 [==============================] - 19s 27ms/step\n","0.2575566480518518\n","576/576 [==============================] - 16s 27ms/step\n","0.29679510877528226\n","38/38 [==============================] - 1s 27ms/step\n","0.011810985853356874\n","conv_20221201-000533_0000_lr0.002_bs512_ep500_inaccessible\n","698/698 [==============================] - 20s 29ms/step\n","0.27102421131250776\n","576/576 [==============================] - 16s 27ms/step\n","0.4040848056012004\n","38/38 [==============================] - 1s 27ms/step\n","-0.03946427284343315\n"]}]},{"cell_type":"code","source":["# create results dataframe\n","results_df = pd.DataFrame()\n","\n","results_df[\"filepath\"] = experiment_paths\n","\n","results_df[\"r2_accessible\"] = np.array(total_r2_results)[:,0]\n","results_df[\"r2_inaccessible\"] = np.array(total_r2_results)[:,1]\n","results_df[\"r2_35s\"] = np.array(total_r2_results)[:,2]\n","\n","results_df[\"spearman_accessible\"] = np.array(total_spearman_results)[:,0]\n","results_df[\"spearman_inaccessible\"] = np.array(total_spearman_results)[:,1]\n","results_df[\"spearman_35s\"] = np.array(total_spearman_results)[:,2]"],"metadata":{"id":"unt_E4fDuUJB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kc30ZsXOeGvt","executionInfo":{"status":"ok","timestamp":1669861795909,"user_tz":300,"elapsed":271,"user":{"displayName":"Sacha Davis","userId":"18177606934486709137"}},"outputId":"edb6642f-9f4f-46e3-9aa0-45ba1b5a15bf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" \u001b[0m\u001b[01;34mdata\u001b[0m/         'Larry Proposal.gdoc'   \u001b[01;34mnotebooks\u001b[0m/   requirements.txt\n"," driver.ipynb   \u001b[01;34mlegacy\u001b[0m/                output.csv   \u001b[01;34msrc\u001b[0m/\n"," \u001b[01;34mexperiments\u001b[0m/   \u001b[01;34mmodels\u001b[0m/                README.md\n"]}]},{"cell_type":"code","source":["results_df"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"npwT7APUgcX3","executionInfo":{"status":"ok","timestamp":1669861795910,"user_tz":300,"elapsed":3,"user":{"displayName":"Sacha Davis","userId":"18177606934486709137"}},"outputId":"edcdd52f-f0c2-4135-be0c-6e7da68dbf1d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                            filepath  r2_accessible  \\\n","0  freq_20221130-234223_nuc101_lay24-64-0-1_lr0.0...       0.130832   \n","1  freq_20221130-234316_nuc101_lay24-64-0-1_lr0.0...       0.077866   \n","2  conv_20221130-234403_0000_lr0.002_bs512_ep500_...       0.007310   \n","3  conv_20221201-000533_0000_lr0.002_bs512_ep500_...       0.005903   \n","\n","   r2_inaccessible    r2_35s  spearman_accessible  spearman_inaccessible  \\\n","0         0.170505 -1.159325             0.386505               0.433519   \n","1         0.272888 -0.999370             0.324562               0.534685   \n","2         0.032410 -7.860244             0.257557               0.296795   \n","3         0.127055 -2.296831             0.271024               0.404085   \n","\n","   spearman_35s  \n","0      0.002890  \n","1      0.006783  \n","2      0.011811  \n","3     -0.039464  "],"text/html":["\n","  <div id=\"df-51b5e943-bc8f-4cbf-91b2-11c98262d3a1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>filepath</th>\n","      <th>r2_accessible</th>\n","      <th>r2_inaccessible</th>\n","      <th>r2_35s</th>\n","      <th>spearman_accessible</th>\n","      <th>spearman_inaccessible</th>\n","      <th>spearman_35s</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>freq_20221130-234223_nuc101_lay24-64-0-1_lr0.0...</td>\n","      <td>0.130832</td>\n","      <td>0.170505</td>\n","      <td>-1.159325</td>\n","      <td>0.386505</td>\n","      <td>0.433519</td>\n","      <td>0.002890</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>freq_20221130-234316_nuc101_lay24-64-0-1_lr0.0...</td>\n","      <td>0.077866</td>\n","      <td>0.272888</td>\n","      <td>-0.999370</td>\n","      <td>0.324562</td>\n","      <td>0.534685</td>\n","      <td>0.006783</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>conv_20221130-234403_0000_lr0.002_bs512_ep500_...</td>\n","      <td>0.007310</td>\n","      <td>0.032410</td>\n","      <td>-7.860244</td>\n","      <td>0.257557</td>\n","      <td>0.296795</td>\n","      <td>0.011811</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>conv_20221201-000533_0000_lr0.002_bs512_ep500_...</td>\n","      <td>0.005903</td>\n","      <td>0.127055</td>\n","      <td>-2.296831</td>\n","      <td>0.271024</td>\n","      <td>0.404085</td>\n","      <td>-0.039464</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-51b5e943-bc8f-4cbf-91b2-11c98262d3a1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-51b5e943-bc8f-4cbf-91b2-11c98262d3a1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-51b5e943-bc8f-4cbf-91b2-11c98262d3a1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["results_df.to_csv(\"experiments/streetlight/conclusive/results.csv\")"],"metadata":{"id":"SmY-Q3xrc3r1"},"execution_count":null,"outputs":[]}]}